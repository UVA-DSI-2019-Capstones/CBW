{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Association Rule: Heuristic Approachimport numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import string\n",
    "import sys\n",
    "from IPython.display import display\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter \n",
    "from itertools import combinations, permutations, groupby\n",
    "\n",
    "# functions to detect bi-grams\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "from nltk.tokenize import word_tokenize #Tokenize words\n",
    "from nltk.stem import WordNetLemmatizer #Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import nltk\n",
    "# nltk.download('wordnet')\n",
    "#nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Reading data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Reading features for each of the paragraphs - Entities and Sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#text_feat = pd.read_csv(\"text_features.csv\",encoding='ISO-8859-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#text_feat.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Reading the actual content of each paragraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CollectionID</th>\n",
       "      <th>BiographyID</th>\n",
       "      <th>ParagraphNo</th>\n",
       "      <th>ParagraphText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a001</td>\n",
       "      <td>bio04</td>\n",
       "      <td>1</td>\n",
       "      <td>A FRENCH philosopher, moralizing on the great ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a001</td>\n",
       "      <td>bio04</td>\n",
       "      <td>2</td>\n",
       "      <td>Cleopatra was joint heir to the throne of Egyp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>a001</td>\n",
       "      <td>bio04</td>\n",
       "      <td>3</td>\n",
       "      <td>Cleopatra might have responded with a brillian...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>a001</td>\n",
       "      <td>bio04</td>\n",
       "      <td>4</td>\n",
       "      <td>Caesar was then above fifty years of age. His ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>a001</td>\n",
       "      <td>bio04</td>\n",
       "      <td>5</td>\n",
       "      <td>For three years Cleopatra reigned with little ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  CollectionID BiographyID  ParagraphNo  \\\n",
       "0         a001       bio04            1   \n",
       "1         a001       bio04            2   \n",
       "2         a001       bio04            3   \n",
       "3         a001       bio04            4   \n",
       "4         a001       bio04            5   \n",
       "\n",
       "                                       ParagraphText  \n",
       "0  A FRENCH philosopher, moralizing on the great ...  \n",
       "1  Cleopatra was joint heir to the throne of Egyp...  \n",
       "2  Cleopatra might have responded with a brillian...  \n",
       "3  Caesar was then above fifty years of age. His ...  \n",
       "4  For three years Cleopatra reigned with little ...  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textdatanew = pd.read_csv(\"textdatanew.csv\",encoding='ISO-8859-1')\n",
    "textdatanew.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Analyzing and Processing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting the paragraph to have each word in a single record <br>\n",
    "Also, using lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordnet_lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word Tokenization and Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Tokenizing the words based on the word tokenizer function in nltk and removing the punctuations\n",
    "textdatanew['Word_split'] = textdatanew.ParagraphText.apply(lambda x: [words for words in word_tokenize(x) if words not in string.punctuation])\n",
    "textdatanew['Word_split_lemma'] = textdatanew.ParagraphText.apply(lambda x: [wordnet_lemmatizer.lemmatize(words) for words in word_tokenize(x) if words not in string.punctuation])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CollectionID</th>\n",
       "      <th>BiographyID</th>\n",
       "      <th>ParagraphNo</th>\n",
       "      <th>ParagraphText</th>\n",
       "      <th>Word_split</th>\n",
       "      <th>Word_split_lemma</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a001</td>\n",
       "      <td>bio04</td>\n",
       "      <td>1</td>\n",
       "      <td>A FRENCH philosopher, moralizing on the great ...</td>\n",
       "      <td>[A, FRENCH, philosopher, moralizing, on, the, ...</td>\n",
       "      <td>[A, FRENCH, philosopher, moralizing, on, the, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a001</td>\n",
       "      <td>bio04</td>\n",
       "      <td>2</td>\n",
       "      <td>Cleopatra was joint heir to the throne of Egyp...</td>\n",
       "      <td>[Cleopatra, was, joint, heir, to, the, throne,...</td>\n",
       "      <td>[Cleopatra, wa, joint, heir, to, the, throne, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>a001</td>\n",
       "      <td>bio04</td>\n",
       "      <td>3</td>\n",
       "      <td>Cleopatra might have responded with a brillian...</td>\n",
       "      <td>[Cleopatra, might, have, responded, with, a, b...</td>\n",
       "      <td>[Cleopatra, might, have, responded, with, a, b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>a001</td>\n",
       "      <td>bio04</td>\n",
       "      <td>4</td>\n",
       "      <td>Caesar was then above fifty years of age. His ...</td>\n",
       "      <td>[Caesar, was, then, above, fifty, years, of, a...</td>\n",
       "      <td>[Caesar, wa, then, above, fifty, year, of, age...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>a001</td>\n",
       "      <td>bio04</td>\n",
       "      <td>5</td>\n",
       "      <td>For three years Cleopatra reigned with little ...</td>\n",
       "      <td>[For, three, years, Cleopatra, reigned, with, ...</td>\n",
       "      <td>[For, three, year, Cleopatra, reigned, with, l...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  CollectionID BiographyID  ParagraphNo  \\\n",
       "0         a001       bio04            1   \n",
       "1         a001       bio04            2   \n",
       "2         a001       bio04            3   \n",
       "3         a001       bio04            4   \n",
       "4         a001       bio04            5   \n",
       "\n",
       "                                       ParagraphText  \\\n",
       "0  A FRENCH philosopher, moralizing on the great ...   \n",
       "1  Cleopatra was joint heir to the throne of Egyp...   \n",
       "2  Cleopatra might have responded with a brillian...   \n",
       "3  Caesar was then above fifty years of age. His ...   \n",
       "4  For three years Cleopatra reigned with little ...   \n",
       "\n",
       "                                          Word_split  \\\n",
       "0  [A, FRENCH, philosopher, moralizing, on, the, ...   \n",
       "1  [Cleopatra, was, joint, heir, to, the, throne,...   \n",
       "2  [Cleopatra, might, have, responded, with, a, b...   \n",
       "3  [Caesar, was, then, above, fifty, years, of, a...   \n",
       "4  [For, three, years, Cleopatra, reigned, with, ...   \n",
       "\n",
       "                                    Word_split_lemma  \n",
       "0  [A, FRENCH, philosopher, moralizing, on, the, ...  \n",
       "1  [Cleopatra, wa, joint, heir, to, the, throne, ...  \n",
       "2  [Cleopatra, might, have, responded, with, a, b...  \n",
       "3  [Caesar, wa, then, above, fifty, year, of, age...  \n",
       "4  [For, three, year, Cleopatra, reigned, with, l...  "
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textdatanew.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bi-gram detector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bi-Gram for tokenized words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_split_sent = textdatanew.Word_split.values\n",
    "bigram = Phrases(word_split_sent, delimiter=b' ',min_count=10)\n",
    "bigram_phraser = Phraser(bigram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bi-Gram for lemmatized words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_lemma_sent = textdatanew.Word_split_lemma.values\n",
    "bigram_lemma = Phrases(word_lemma_sent, delimiter=b' ',min_count=10)\n",
    "bigram_phraser_lemma = Phraser(bigram_lemma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Detecting bi-grams and adding a new column with bi-grams included"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [],
   "source": [
    "textdatanew['Word_split_new'] = textdatanew.Word_split.apply(lambda x: bigram_phraser[x])\n",
    "textdatanew['Word_split_POS'] = textdatanew.Word_split_new.apply(lambda x: nltk.pos_tag(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CollectionID</th>\n",
       "      <th>BiographyID</th>\n",
       "      <th>ParagraphNo</th>\n",
       "      <th>ParagraphText</th>\n",
       "      <th>Word_split</th>\n",
       "      <th>Word_split_lemma</th>\n",
       "      <th>Word_split_new</th>\n",
       "      <th>Word_split_lemma_new</th>\n",
       "      <th>Unique_id</th>\n",
       "      <th>Word_split_POS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a001</td>\n",
       "      <td>bio04</td>\n",
       "      <td>1</td>\n",
       "      <td>A FRENCH philosopher, moralizing on the great ...</td>\n",
       "      <td>[A, FRENCH, philosopher, moralizing, on, the, ...</td>\n",
       "      <td>[A, FRENCH, philosopher, moralizing, on, the, ...</td>\n",
       "      <td>[A, FRENCH, philosopher, moralizing, on, the, ...</td>\n",
       "      <td>[A, FRENCH, philosopher, moralizing, on, the, ...</td>\n",
       "      <td>a001_bio04_1</td>\n",
       "      <td>[(A, DT), (FRENCH, JJ), (philosopher, NN), (mo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a001</td>\n",
       "      <td>bio04</td>\n",
       "      <td>2</td>\n",
       "      <td>Cleopatra was joint heir to the throne of Egyp...</td>\n",
       "      <td>[Cleopatra, was, joint, heir, to, the, throne,...</td>\n",
       "      <td>[Cleopatra, wa, joint, heir, to, the, throne, ...</td>\n",
       "      <td>[Cleopatra, was, joint, heir, to, the, throne,...</td>\n",
       "      <td>[Cleopatra, wa, joint, heir, to, the, throne, ...</td>\n",
       "      <td>a001_bio04_2</td>\n",
       "      <td>[(Cleopatra, NNP), (was, VBD), (joint, JJ), (h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>a001</td>\n",
       "      <td>bio04</td>\n",
       "      <td>3</td>\n",
       "      <td>Cleopatra might have responded with a brillian...</td>\n",
       "      <td>[Cleopatra, might, have, responded, with, a, b...</td>\n",
       "      <td>[Cleopatra, might, have, responded, with, a, b...</td>\n",
       "      <td>[Cleopatra, might have, responded, with, a, br...</td>\n",
       "      <td>[Cleopatra, might have, responded, with, a, br...</td>\n",
       "      <td>a001_bio04_3</td>\n",
       "      <td>[(Cleopatra, NNP), (might have, NN), (responde...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>a001</td>\n",
       "      <td>bio04</td>\n",
       "      <td>4</td>\n",
       "      <td>Caesar was then above fifty years of age. His ...</td>\n",
       "      <td>[Caesar, was, then, above, fifty, years, of, a...</td>\n",
       "      <td>[Caesar, wa, then, above, fifty, year, of, age...</td>\n",
       "      <td>[Caesar, was, then, above, fifty years, of, ag...</td>\n",
       "      <td>[Caesar, wa, then, above, fifty year, of, age,...</td>\n",
       "      <td>a001_bio04_4</td>\n",
       "      <td>[(Caesar, NNP), (was, VBD), (then, RB), (above...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>a001</td>\n",
       "      <td>bio04</td>\n",
       "      <td>5</td>\n",
       "      <td>For three years Cleopatra reigned with little ...</td>\n",
       "      <td>[For, three, years, Cleopatra, reigned, with, ...</td>\n",
       "      <td>[For, three, year, Cleopatra, reigned, with, l...</td>\n",
       "      <td>[For, three years, Cleopatra, reigned, with, l...</td>\n",
       "      <td>[For, three year, Cleopatra, reigned, with, li...</td>\n",
       "      <td>a001_bio04_5</td>\n",
       "      <td>[(For, IN), (three years, NNS), (Cleopatra, NN...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  CollectionID BiographyID  ParagraphNo  \\\n",
       "0         a001       bio04            1   \n",
       "1         a001       bio04            2   \n",
       "2         a001       bio04            3   \n",
       "3         a001       bio04            4   \n",
       "4         a001       bio04            5   \n",
       "\n",
       "                                       ParagraphText  \\\n",
       "0  A FRENCH philosopher, moralizing on the great ...   \n",
       "1  Cleopatra was joint heir to the throne of Egyp...   \n",
       "2  Cleopatra might have responded with a brillian...   \n",
       "3  Caesar was then above fifty years of age. His ...   \n",
       "4  For three years Cleopatra reigned with little ...   \n",
       "\n",
       "                                          Word_split  \\\n",
       "0  [A, FRENCH, philosopher, moralizing, on, the, ...   \n",
       "1  [Cleopatra, was, joint, heir, to, the, throne,...   \n",
       "2  [Cleopatra, might, have, responded, with, a, b...   \n",
       "3  [Caesar, was, then, above, fifty, years, of, a...   \n",
       "4  [For, three, years, Cleopatra, reigned, with, ...   \n",
       "\n",
       "                                    Word_split_lemma  \\\n",
       "0  [A, FRENCH, philosopher, moralizing, on, the, ...   \n",
       "1  [Cleopatra, wa, joint, heir, to, the, throne, ...   \n",
       "2  [Cleopatra, might, have, responded, with, a, b...   \n",
       "3  [Caesar, wa, then, above, fifty, year, of, age...   \n",
       "4  [For, three, year, Cleopatra, reigned, with, l...   \n",
       "\n",
       "                                      Word_split_new  \\\n",
       "0  [A, FRENCH, philosopher, moralizing, on, the, ...   \n",
       "1  [Cleopatra, was, joint, heir, to, the, throne,...   \n",
       "2  [Cleopatra, might have, responded, with, a, br...   \n",
       "3  [Caesar, was, then, above, fifty years, of, ag...   \n",
       "4  [For, three years, Cleopatra, reigned, with, l...   \n",
       "\n",
       "                                Word_split_lemma_new     Unique_id  \\\n",
       "0  [A, FRENCH, philosopher, moralizing, on, the, ...  a001_bio04_1   \n",
       "1  [Cleopatra, wa, joint, heir, to, the, throne, ...  a001_bio04_2   \n",
       "2  [Cleopatra, might have, responded, with, a, br...  a001_bio04_3   \n",
       "3  [Caesar, wa, then, above, fifty year, of, age,...  a001_bio04_4   \n",
       "4  [For, three year, Cleopatra, reigned, with, li...  a001_bio04_5   \n",
       "\n",
       "                                      Word_split_POS  \n",
       "0  [(A, DT), (FRENCH, JJ), (philosopher, NN), (mo...  \n",
       "1  [(Cleopatra, NNP), (was, VBD), (joint, JJ), (h...  \n",
       "2  [(Cleopatra, NNP), (might have, NN), (responde...  \n",
       "3  [(Caesar, NNP), (was, VBD), (then, RB), (above...  \n",
       "4  [(For, IN), (three years, NNS), (Cleopatra, NN...  "
      ]
     },
     "execution_count": 339,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textdatanew.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "textdatanew['Word_split_lemma_new'] = textdatanew.Word_split_lemma.apply(lambda x: bigram_phraser_lemma[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CollectionID</th>\n",
       "      <th>BiographyID</th>\n",
       "      <th>ParagraphNo</th>\n",
       "      <th>ParagraphText</th>\n",
       "      <th>Word_split</th>\n",
       "      <th>Word_split_lemma</th>\n",
       "      <th>Word_split_new</th>\n",
       "      <th>Word_split_lemma_new</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a001</td>\n",
       "      <td>bio04</td>\n",
       "      <td>1</td>\n",
       "      <td>A FRENCH philosopher, moralizing on the great ...</td>\n",
       "      <td>[A, FRENCH, philosopher, moralizing, on, the, ...</td>\n",
       "      <td>[A, FRENCH, philosopher, moralizing, on, the, ...</td>\n",
       "      <td>[A, FRENCH, philosopher, moralizing, on, the, ...</td>\n",
       "      <td>[A, FRENCH, philosopher, moralizing, on, the, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a001</td>\n",
       "      <td>bio04</td>\n",
       "      <td>2</td>\n",
       "      <td>Cleopatra was joint heir to the throne of Egyp...</td>\n",
       "      <td>[Cleopatra, was, joint, heir, to, the, throne,...</td>\n",
       "      <td>[Cleopatra, wa, joint, heir, to, the, throne, ...</td>\n",
       "      <td>[Cleopatra, was, joint, heir, to, the, throne,...</td>\n",
       "      <td>[Cleopatra, wa, joint, heir, to, the, throne, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>a001</td>\n",
       "      <td>bio04</td>\n",
       "      <td>3</td>\n",
       "      <td>Cleopatra might have responded with a brillian...</td>\n",
       "      <td>[Cleopatra, might, have, responded, with, a, b...</td>\n",
       "      <td>[Cleopatra, might, have, responded, with, a, b...</td>\n",
       "      <td>[Cleopatra, might have, responded, with, a, br...</td>\n",
       "      <td>[Cleopatra, might have, responded, with, a, br...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>a001</td>\n",
       "      <td>bio04</td>\n",
       "      <td>4</td>\n",
       "      <td>Caesar was then above fifty years of age. His ...</td>\n",
       "      <td>[Caesar, was, then, above, fifty, years, of, a...</td>\n",
       "      <td>[Caesar, wa, then, above, fifty, year, of, age...</td>\n",
       "      <td>[Caesar, was, then, above, fifty years, of, ag...</td>\n",
       "      <td>[Caesar, wa, then, above, fifty year, of, age,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>a001</td>\n",
       "      <td>bio04</td>\n",
       "      <td>5</td>\n",
       "      <td>For three years Cleopatra reigned with little ...</td>\n",
       "      <td>[For, three, years, Cleopatra, reigned, with, ...</td>\n",
       "      <td>[For, three, year, Cleopatra, reigned, with, l...</td>\n",
       "      <td>[For three, years, Cleopatra, reigned, with, l...</td>\n",
       "      <td>[For three, year, Cleopatra, reigned, with, li...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  CollectionID BiographyID  ParagraphNo  \\\n",
       "0         a001       bio04            1   \n",
       "1         a001       bio04            2   \n",
       "2         a001       bio04            3   \n",
       "3         a001       bio04            4   \n",
       "4         a001       bio04            5   \n",
       "\n",
       "                                       ParagraphText  \\\n",
       "0  A FRENCH philosopher, moralizing on the great ...   \n",
       "1  Cleopatra was joint heir to the throne of Egyp...   \n",
       "2  Cleopatra might have responded with a brillian...   \n",
       "3  Caesar was then above fifty years of age. His ...   \n",
       "4  For three years Cleopatra reigned with little ...   \n",
       "\n",
       "                                          Word_split  \\\n",
       "0  [A, FRENCH, philosopher, moralizing, on, the, ...   \n",
       "1  [Cleopatra, was, joint, heir, to, the, throne,...   \n",
       "2  [Cleopatra, might, have, responded, with, a, b...   \n",
       "3  [Caesar, was, then, above, fifty, years, of, a...   \n",
       "4  [For, three, years, Cleopatra, reigned, with, ...   \n",
       "\n",
       "                                    Word_split_lemma  \\\n",
       "0  [A, FRENCH, philosopher, moralizing, on, the, ...   \n",
       "1  [Cleopatra, wa, joint, heir, to, the, throne, ...   \n",
       "2  [Cleopatra, might, have, responded, with, a, b...   \n",
       "3  [Caesar, wa, then, above, fifty, year, of, age...   \n",
       "4  [For, three, year, Cleopatra, reigned, with, l...   \n",
       "\n",
       "                                      Word_split_new  \\\n",
       "0  [A, FRENCH, philosopher, moralizing, on, the, ...   \n",
       "1  [Cleopatra, was, joint, heir, to, the, throne,...   \n",
       "2  [Cleopatra, might have, responded, with, a, br...   \n",
       "3  [Caesar, was, then, above, fifty years, of, ag...   \n",
       "4  [For three, years, Cleopatra, reigned, with, l...   \n",
       "\n",
       "                                Word_split_lemma_new  \n",
       "0  [A, FRENCH, philosopher, moralizing, on, the, ...  \n",
       "1  [Cleopatra, wa, joint, heir, to, the, throne, ...  \n",
       "2  [Cleopatra, might have, responded, with, a, br...  \n",
       "3  [Caesar, wa, then, above, fifty year, of, age,...  \n",
       "4  [For three, year, Cleopatra, reigned, with, li...  "
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textdatanew.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CollectionID</th>\n",
       "      <th>BiographyID</th>\n",
       "      <th>ParagraphNo</th>\n",
       "      <th>ParagraphText</th>\n",
       "      <th>Word_split</th>\n",
       "      <th>Word_split_lemma</th>\n",
       "      <th>Word_split_new</th>\n",
       "      <th>Word_split_lemma_new</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a001</td>\n",
       "      <td>bio04</td>\n",
       "      <td>1</td>\n",
       "      <td>A FRENCH philosopher, moralizing on the great ...</td>\n",
       "      <td>[A, FRENCH, philosopher, moralizing, on, the, ...</td>\n",
       "      <td>[A, FRENCH, philosopher, moralizing, on, the, ...</td>\n",
       "      <td>[A, FRENCH, philosopher, moralizing, on, the, ...</td>\n",
       "      <td>[A, FRENCH, philosopher, moralizing, on, the, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a001</td>\n",
       "      <td>bio04</td>\n",
       "      <td>2</td>\n",
       "      <td>Cleopatra was joint heir to the throne of Egyp...</td>\n",
       "      <td>[Cleopatra, was, joint, heir, to, the, throne,...</td>\n",
       "      <td>[Cleopatra, wa, joint, heir, to, the, throne, ...</td>\n",
       "      <td>[Cleopatra, was, joint, heir, to, the, throne,...</td>\n",
       "      <td>[Cleopatra, wa, joint, heir, to, the, throne, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>a001</td>\n",
       "      <td>bio04</td>\n",
       "      <td>3</td>\n",
       "      <td>Cleopatra might have responded with a brillian...</td>\n",
       "      <td>[Cleopatra, might, have, responded, with, a, b...</td>\n",
       "      <td>[Cleopatra, might, have, responded, with, a, b...</td>\n",
       "      <td>[Cleopatra, might have, responded, with, a, br...</td>\n",
       "      <td>[Cleopatra, might have, responded, with, a, br...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>a001</td>\n",
       "      <td>bio04</td>\n",
       "      <td>4</td>\n",
       "      <td>Caesar was then above fifty years of age. His ...</td>\n",
       "      <td>[Caesar, was, then, above, fifty, years, of, a...</td>\n",
       "      <td>[Caesar, wa, then, above, fifty, year, of, age...</td>\n",
       "      <td>[Caesar, was, then, above, fifty years, of, ag...</td>\n",
       "      <td>[Caesar, wa, then, above, fifty year, of, age,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>a001</td>\n",
       "      <td>bio04</td>\n",
       "      <td>5</td>\n",
       "      <td>For three years Cleopatra reigned with little ...</td>\n",
       "      <td>[For, three, years, Cleopatra, reigned, with, ...</td>\n",
       "      <td>[For, three, year, Cleopatra, reigned, with, l...</td>\n",
       "      <td>[For, three years, Cleopatra, reigned, with, l...</td>\n",
       "      <td>[For, three year, Cleopatra, reigned, with, li...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  CollectionID BiographyID  ParagraphNo  \\\n",
       "0         a001       bio04            1   \n",
       "1         a001       bio04            2   \n",
       "2         a001       bio04            3   \n",
       "3         a001       bio04            4   \n",
       "4         a001       bio04            5   \n",
       "\n",
       "                                       ParagraphText  \\\n",
       "0  A FRENCH philosopher, moralizing on the great ...   \n",
       "1  Cleopatra was joint heir to the throne of Egyp...   \n",
       "2  Cleopatra might have responded with a brillian...   \n",
       "3  Caesar was then above fifty years of age. His ...   \n",
       "4  For three years Cleopatra reigned with little ...   \n",
       "\n",
       "                                          Word_split  \\\n",
       "0  [A, FRENCH, philosopher, moralizing, on, the, ...   \n",
       "1  [Cleopatra, was, joint, heir, to, the, throne,...   \n",
       "2  [Cleopatra, might, have, responded, with, a, b...   \n",
       "3  [Caesar, was, then, above, fifty, years, of, a...   \n",
       "4  [For, three, years, Cleopatra, reigned, with, ...   \n",
       "\n",
       "                                    Word_split_lemma  \\\n",
       "0  [A, FRENCH, philosopher, moralizing, on, the, ...   \n",
       "1  [Cleopatra, wa, joint, heir, to, the, throne, ...   \n",
       "2  [Cleopatra, might, have, responded, with, a, b...   \n",
       "3  [Caesar, wa, then, above, fifty, year, of, age...   \n",
       "4  [For, three, year, Cleopatra, reigned, with, l...   \n",
       "\n",
       "                                      Word_split_new  \\\n",
       "0  [A, FRENCH, philosopher, moralizing, on, the, ...   \n",
       "1  [Cleopatra, was, joint, heir, to, the, throne,...   \n",
       "2  [Cleopatra, might have, responded, with, a, br...   \n",
       "3  [Caesar, was, then, above, fifty years, of, ag...   \n",
       "4  [For, three years, Cleopatra, reigned, with, l...   \n",
       "\n",
       "                                Word_split_lemma_new  \n",
       "0  [A, FRENCH, philosopher, moralizing, on, the, ...  \n",
       "1  [Cleopatra, wa, joint, heir, to, the, throne, ...  \n",
       "2  [Cleopatra, might have, responded, with, a, br...  \n",
       "3  [Caesar, wa, then, above, fifty year, of, age,...  \n",
       "4  [For, three year, Cleopatra, reigned, with, li...  "
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textdatanew.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating Unique Id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "textdatanew['Unique_id'] = textdatanew.CollectionID+\"_\"+textdatanew.BiographyID+\"_\"+textdatanew.ParagraphNo.apply(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CollectionID</th>\n",
       "      <th>BiographyID</th>\n",
       "      <th>ParagraphNo</th>\n",
       "      <th>ParagraphText</th>\n",
       "      <th>Word_split</th>\n",
       "      <th>Word_split_lemma</th>\n",
       "      <th>Word_split_new</th>\n",
       "      <th>Word_split_lemma_new</th>\n",
       "      <th>Unique_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a001</td>\n",
       "      <td>bio04</td>\n",
       "      <td>1</td>\n",
       "      <td>A FRENCH philosopher, moralizing on the great ...</td>\n",
       "      <td>[A, FRENCH, philosopher, moralizing, on, the, ...</td>\n",
       "      <td>[A, FRENCH, philosopher, moralizing, on, the, ...</td>\n",
       "      <td>[A, FRENCH, philosopher, moralizing, on, the, ...</td>\n",
       "      <td>[A, FRENCH, philosopher, moralizing, on, the, ...</td>\n",
       "      <td>a001_bio04_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a001</td>\n",
       "      <td>bio04</td>\n",
       "      <td>2</td>\n",
       "      <td>Cleopatra was joint heir to the throne of Egyp...</td>\n",
       "      <td>[Cleopatra, was, joint, heir, to, the, throne,...</td>\n",
       "      <td>[Cleopatra, wa, joint, heir, to, the, throne, ...</td>\n",
       "      <td>[Cleopatra, was, joint, heir, to, the, throne,...</td>\n",
       "      <td>[Cleopatra, wa, joint, heir, to, the, throne, ...</td>\n",
       "      <td>a001_bio04_2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>a001</td>\n",
       "      <td>bio04</td>\n",
       "      <td>3</td>\n",
       "      <td>Cleopatra might have responded with a brillian...</td>\n",
       "      <td>[Cleopatra, might, have, responded, with, a, b...</td>\n",
       "      <td>[Cleopatra, might, have, responded, with, a, b...</td>\n",
       "      <td>[Cleopatra, might have, responded, with, a, br...</td>\n",
       "      <td>[Cleopatra, might have, responded, with, a, br...</td>\n",
       "      <td>a001_bio04_3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>a001</td>\n",
       "      <td>bio04</td>\n",
       "      <td>4</td>\n",
       "      <td>Caesar was then above fifty years of age. His ...</td>\n",
       "      <td>[Caesar, was, then, above, fifty, years, of, a...</td>\n",
       "      <td>[Caesar, wa, then, above, fifty, year, of, age...</td>\n",
       "      <td>[Caesar, was, then, above, fifty years, of, ag...</td>\n",
       "      <td>[Caesar, wa, then, above, fifty year, of, age,...</td>\n",
       "      <td>a001_bio04_4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>a001</td>\n",
       "      <td>bio04</td>\n",
       "      <td>5</td>\n",
       "      <td>For three years Cleopatra reigned with little ...</td>\n",
       "      <td>[For, three, years, Cleopatra, reigned, with, ...</td>\n",
       "      <td>[For, three, year, Cleopatra, reigned, with, l...</td>\n",
       "      <td>[For, three years, Cleopatra, reigned, with, l...</td>\n",
       "      <td>[For, three year, Cleopatra, reigned, with, li...</td>\n",
       "      <td>a001_bio04_5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  CollectionID BiographyID  ParagraphNo  \\\n",
       "0         a001       bio04            1   \n",
       "1         a001       bio04            2   \n",
       "2         a001       bio04            3   \n",
       "3         a001       bio04            4   \n",
       "4         a001       bio04            5   \n",
       "\n",
       "                                       ParagraphText  \\\n",
       "0  A FRENCH philosopher, moralizing on the great ...   \n",
       "1  Cleopatra was joint heir to the throne of Egyp...   \n",
       "2  Cleopatra might have responded with a brillian...   \n",
       "3  Caesar was then above fifty years of age. His ...   \n",
       "4  For three years Cleopatra reigned with little ...   \n",
       "\n",
       "                                          Word_split  \\\n",
       "0  [A, FRENCH, philosopher, moralizing, on, the, ...   \n",
       "1  [Cleopatra, was, joint, heir, to, the, throne,...   \n",
       "2  [Cleopatra, might, have, responded, with, a, b...   \n",
       "3  [Caesar, was, then, above, fifty, years, of, a...   \n",
       "4  [For, three, years, Cleopatra, reigned, with, ...   \n",
       "\n",
       "                                    Word_split_lemma  \\\n",
       "0  [A, FRENCH, philosopher, moralizing, on, the, ...   \n",
       "1  [Cleopatra, wa, joint, heir, to, the, throne, ...   \n",
       "2  [Cleopatra, might, have, responded, with, a, b...   \n",
       "3  [Caesar, wa, then, above, fifty, year, of, age...   \n",
       "4  [For, three, year, Cleopatra, reigned, with, l...   \n",
       "\n",
       "                                      Word_split_new  \\\n",
       "0  [A, FRENCH, philosopher, moralizing, on, the, ...   \n",
       "1  [Cleopatra, was, joint, heir, to, the, throne,...   \n",
       "2  [Cleopatra, might have, responded, with, a, br...   \n",
       "3  [Caesar, was, then, above, fifty years, of, ag...   \n",
       "4  [For, three years, Cleopatra, reigned, with, l...   \n",
       "\n",
       "                                Word_split_lemma_new     Unique_id  \n",
       "0  [A, FRENCH, philosopher, moralizing, on, the, ...  a001_bio04_1  \n",
       "1  [Cleopatra, wa, joint, heir, to, the, throne, ...  a001_bio04_2  \n",
       "2  [Cleopatra, might have, responded, with, a, br...  a001_bio04_3  \n",
       "3  [Caesar, wa, then, above, fifty year, of, age,...  a001_bio04_4  \n",
       "4  [For, three year, Cleopatra, reigned, with, li...  a001_bio04_5  "
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textdatanew.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stacking the text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Words without lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_stack = textdatanew.apply(lambda x: pd.Series(x['Word_split_new']), axis=1).stack().reset_index(level=1, drop=True)\n",
    "text_stack.name = 'Word_splits'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [],
   "source": [
    "stack_cols = ['Unique_id']\n",
    "text_data_words = textdatanew[stack_cols].join(text_stack)\n",
    "#text_data_words['Word_splits'] = pd.Series(textdatanew2['Word_splits'], dtype=object)\n",
    "\n",
    "## Character length\n",
    "text_data_words['char_len'] = text_data_words.Word_splits.apply(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unique_id</th>\n",
       "      <th>Word_splits</th>\n",
       "      <th>char_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a001_bio04_1</td>\n",
       "      <td>A</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a001_bio04_1</td>\n",
       "      <td>FRENCH</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a001_bio04_1</td>\n",
       "      <td>philosopher</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a001_bio04_1</td>\n",
       "      <td>moralizing</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a001_bio04_1</td>\n",
       "      <td>on</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unique_id  Word_splits  char_len\n",
       "0  a001_bio04_1            A         1\n",
       "0  a001_bio04_1       FRENCH         6\n",
       "0  a001_bio04_1  philosopher        11\n",
       "0  a001_bio04_1   moralizing        10\n",
       "0  a001_bio04_1           on         2"
      ]
     },
     "execution_count": 341,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_data_words.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Lemmatized words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_stack_lemma = textdatanew.apply(lambda x: pd.Series(x['Word_split_lemma_new']), axis=1).stack().reset_index(level=1, drop=True)\n",
    "text_stack_lemma.name = 'Word_split_lemma'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "stack_cols = ['Unique_id']\n",
    "text_data_words_lemma = textdatanew[stack_cols].join(text_stack_lemma)\n",
    "text_data_words_lemma['Word_split_lemma'] = pd.Series(text_data_words_lemma['Word_split_lemma'], dtype=object)\n",
    "\n",
    "## Character length\n",
    "text_data_words_lemma['char_len'] = text_data_words_lemma.Word_split_lemma.apply(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unique_id</th>\n",
       "      <th>Word_split_lemma</th>\n",
       "      <th>char_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a001_bio04_1</td>\n",
       "      <td>A</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a001_bio04_1</td>\n",
       "      <td>FRENCH</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a001_bio04_1</td>\n",
       "      <td>philosopher</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a001_bio04_1</td>\n",
       "      <td>moralizing</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a001_bio04_1</td>\n",
       "      <td>on</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unique_id Word_split_lemma  char_len\n",
       "0  a001_bio04_1                A         1\n",
       "0  a001_bio04_1           FRENCH         6\n",
       "0  a001_bio04_1      philosopher        11\n",
       "0  a001_bio04_1       moralizing        10\n",
       "0  a001_bio04_1               on         2"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_data_words_lemma.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a Unique identifier for each of the paragraph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identifying and Removing Redundant words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Tokenized Words\n",
    "word_counts = Counter(text_data_words.Word_splits.apply(lambda x: x.lower()))\n",
    "most_common_words = [each[0] for each in word_counts.most_common(100)]\n",
    "\n",
    "### Lemmatized Words\n",
    "word_counts_lemm = Counter(text_data_words_lemma.Word_split_lemma.apply(lambda x: x.lower()))\n",
    "most_common_words_lemm = [each[0] for each in word_counts.most_common(100)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_data_words['Word_freq'] = text_data_words.Word_splits.apply(lambda x: word_counts[x.lower()])\n",
    "text_data_words_lemma['Word_freq'] = text_data_words_lemma.Word_split_lemma.apply(lambda x: word_counts_lemm[x.lower()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words_list = stopwords.words('english')\n",
    "\n",
    "### Adding the most common words\n",
    "stop_words_list.extend(most_common_words)\n",
    "stop_words_list.extend(most_common_words_lemm)\n",
    "\n",
    "## Adding the least frequenct words\n",
    "less_freq_words = text_data_words[text_data_words.Word_freq < 10].Word_splits.str.lower().values\n",
    "less_freq_words_lemm = text_data_words_lemma[text_data_words_lemma.Word_freq < 10].Word_split_lemma.str.lower().values\n",
    "\n",
    "\n",
    "stop_words_list.extend(less_freq_words)\n",
    "stop_words_list.extend(less_freq_words_lemm)\n",
    "\n",
    "#### Removing words with less than or equal to two characters and making sure they are not numeric\n",
    "words_2_char = text_data_words[text_data_words.char_len < 3].Word_splits.unique().tolist()\n",
    "words_2_char_l = text_data_words_lemma[text_data_words_lemma.char_len < 3].Word_split_lemma.unique().tolist()\n",
    "\n",
    "words_2_char.extend(words_2_char_l)\n",
    "words_2_char.extend(words_2_char)\n",
    "\n",
    "word_2_char_to_remove = [each for each in words_2_char if ~each.isdigit()]\n",
    "\n",
    "stop_words_list.extend(word_2_char_to_remove)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing the stop words from the original data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "Text_final = text_data_words[~text_data_words.Word_splits.str.lower().isin(stop_words_list)]\n",
    "Text_final_lemma = text_data_words_lemma[~text_data_words_lemma.Word_split_lemma.str.lower().isin(stop_words_list)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\arvra\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  after removing the cwd from sys.path.\n",
      "C:\\Users\\arvra\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "# Text_para_final.loc[:,'Words_clean'] = Text_para_final.Word_splits.apply(lambda x: x.lower())\n",
    "# Text_Para.loc[:,'Words_clean'] = Text_Para.Word_splits.apply(lambda x: x.lower())\n",
    "\n",
    "Text_final['Words_clean'] = Text_final.Word_splits.apply(lambda x: x.lower()).values\n",
    "Text_final_lemma['Words_clean'] = Text_final_lemma.Word_split_lemma.apply(lambda x: x.lower()).values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribution of words greater than minimum and less than maximum threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [],
   "source": [
    "#p = Text_Para.Word_freq[(Text_Para.Word_freq>20) & (Text_Para.Word_freq < 1500)].hist(bins = 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Association Rule Mining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns frequency counts for items and item pairs\n",
    "def freq(iterable):\n",
    "    if type(iterable) == pd.core.series.Series:\n",
    "        return iterable.value_counts().rename(\"freq\")\n",
    "    else: \n",
    "        return pd.Series(Counter(iterable)).rename(\"freq\")\n",
    "\n",
    "    \n",
    "# Returns number of unique orders\n",
    "def order_count(order_item):\n",
    "    return len(set(order_item.index))\n",
    "\n",
    "\n",
    "# Returns generator that yields item pairs, one at a time\n",
    "def get_item_pairs(order_item):\n",
    "    order_item = order_item.reset_index().as_matrix()\n",
    "    #print(order_item)\n",
    "    for order_id, order_object in groupby(order_item, lambda x: x[0]):\n",
    "        item_list = [item[1] for item in order_object]\n",
    "              \n",
    "        for item_pair in combinations(item_list, 2):\n",
    "            yield item_pair\n",
    "            \n",
    "\n",
    "# Returns frequency and support associated with item\n",
    "def merge_item_stats(item_pairs, item_stats):\n",
    "    return (item_pairs\n",
    "                .merge(item_stats.rename(columns={'freq': 'freqA', 'support': 'supportA'}), left_on='item_A', right_index=True)\n",
    "                .merge(item_stats.rename(columns={'freq': 'freqB', 'support': 'supportB'}), left_on='item_B', right_index=True))\n",
    "\n",
    "\n",
    "# Returns name associated with item\n",
    "def merge_item_name(rules, item_name):\n",
    "    columns = ['itemA','itemB','freqAB','supportAB','freqA','supportA','freqB','supportB', \n",
    "               'confidenceAtoB','confidenceBtoA','lift']\n",
    "    rules = (rules\n",
    "                .merge(item_name.rename(columns={'item_name': 'itemA'}), left_on='item_A', right_on='item_id')\n",
    "                .merge(item_name.rename(columns={'item_name': 'itemB'}), left_on='item_B', right_on='item_id'))\n",
    "    return rules[columns]   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "def association_rules(order_item, min_support):\n",
    "\n",
    "    print(\"Starting order_item: {:22d}\".format(len(order_item)))\n",
    "\n",
    "\n",
    "    # Calculate item frequency and support\n",
    "    item_stats             = freq(order_item).to_frame(\"freq\")\n",
    "    item_stats['support']  = item_stats['freq'] / order_count(order_item) * 100\n",
    "\n",
    "\n",
    "    # Filter from order_item items below min support \n",
    "    qualifying_items       = item_stats[item_stats['support'] >= min_support].index\n",
    "    order_item             = order_item[order_item.isin(qualifying_items)]\n",
    "\n",
    "    print(\"Items with support >= {}: {:15d}\".format(min_support, len(qualifying_items)))\n",
    "    print(\"Remaining order_item: {:21d}\".format(len(order_item)))\n",
    "\n",
    "\n",
    "    # Filter from order_item orders with less than 2 items\n",
    "    order_size             = freq(order_item.index)\n",
    "    qualifying_orders      = order_size[order_size >= 2].index\n",
    "    order_item             = order_item[order_item.index.isin(qualifying_orders)]\n",
    "\n",
    "    print(\"Remaining orders with 2+ items: {:11d}\".format(len(qualifying_orders)))\n",
    "    print(\"Remaining order_item: {:21d}\".format(len(order_item)))\n",
    "\n",
    "\n",
    "    # Recalculate item frequency and support\n",
    "    item_stats             = freq(order_item).to_frame(\"freq\")\n",
    "    item_stats['support']  = item_stats['freq'] / order_count(order_item) * 100\n",
    "\n",
    "\n",
    "    # Get item pairs generator\n",
    "    item_pair_gen          = get_item_pairs(order_item)\n",
    "\n",
    "\n",
    "    # Calculate item pair frequency and support\n",
    "    item_pairs              = freq(item_pair_gen).to_frame(\"freqAB\")\n",
    "    item_pairs['supportAB'] = item_pairs['freqAB'] / len(qualifying_orders) * 100\n",
    "\n",
    "    print(\"Item pairs: {:31d}\".format(len(item_pairs)))\n",
    "\n",
    "\n",
    "    # Filter from item_pairs those below min support\n",
    "    item_pairs              = item_pairs[item_pairs['supportAB'] >= min_support]\n",
    "\n",
    "    print(\"Item pairs with support >= {}: {:10d}\\n\".format(min_support, len(item_pairs)))\n",
    "\n",
    "\n",
    "    # Create table of association rules and compute relevant metrics\n",
    "    item_pairs = item_pairs.reset_index().rename(columns={'level_0': 'item_A', 'level_1': 'item_B'})\n",
    "    item_pairs = merge_item_stats(item_pairs, item_stats)\n",
    "    \n",
    "    item_pairs['confidenceAtoB'] = item_pairs['supportAB'] / item_pairs['supportA']\n",
    "    item_pairs['confidenceBtoA'] = item_pairs['supportAB'] / item_pairs['supportB']\n",
    "    item_pairs['lift']           = item_pairs['supportAB'] / (item_pairs['supportA'] * item_pairs['supportB'])\n",
    "    \n",
    "    \n",
    "    # Return association rules sorted by lift in descending order\n",
    "    return item_pairs.sort_values('lift', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying Association Rule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "Words = Text_final_lemma[['Unique_id','Words_clean']].drop_duplicates().set_index('Unique_id')['Words_clean'].rename('item_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting order_item:                 620471\n",
      "Items with support >= 0.01:            9958\n",
      "Remaining order_item:                620471\n",
      "Remaining orders with 2+ items:       16526\n",
      "Remaining order_item:                620383\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m<ipython-input-278-2ec380323e4f>\u001b[0m in \u001b[0;36massociation_rules\u001b[1;34m(order_item, min_support)\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m     \u001b[1;31m# Calculate item pair frequency and support\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 38\u001b[1;33m     \u001b[0mitem_pairs\u001b[0m              \u001b[1;33m=\u001b[0m \u001b[0mfreq\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem_pair_gen\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_frame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"freqAB\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     39\u001b[0m     \u001b[0mitem_pairs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'supportAB'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mitem_pairs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'freqAB'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mqualifying_orders\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-277-4c5193a3ba4c>\u001b[0m in \u001b[0;36mfreq\u001b[1;34m(iterable)\u001b[0m\n\u001b[0;32m      4\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0miterable\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue_counts\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrename\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"freq\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSeries\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mCounter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterable\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrename\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"freq\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "rules = association_rules(Words, 0.01)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>item_A</th>\n",
       "      <th>item_B</th>\n",
       "      <th>freqAB</th>\n",
       "      <th>supportAB</th>\n",
       "      <th>freqA</th>\n",
       "      <th>supportA</th>\n",
       "      <th>freqB</th>\n",
       "      <th>supportB</th>\n",
       "      <th>confidenceAtoB</th>\n",
       "      <th>confidenceBtoA</th>\n",
       "      <th>lift</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1155232</th>\n",
       "      <td>france</td>\n",
       "      <td>maria-theresa</td>\n",
       "      <td>6</td>\n",
       "      <td>0.036280</td>\n",
       "      <td>435</td>\n",
       "      <td>2.630306</td>\n",
       "      <td>10</td>\n",
       "      <td>0.060467</td>\n",
       "      <td>0.013793</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.228110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1154266</th>\n",
       "      <td>france</td>\n",
       "      <td>conti</td>\n",
       "      <td>6</td>\n",
       "      <td>0.036280</td>\n",
       "      <td>435</td>\n",
       "      <td>2.630306</td>\n",
       "      <td>10</td>\n",
       "      <td>0.060467</td>\n",
       "      <td>0.013793</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.228110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1154237</th>\n",
       "      <td>france</td>\n",
       "      <td>cond</td>\n",
       "      <td>7</td>\n",
       "      <td>0.042327</td>\n",
       "      <td>435</td>\n",
       "      <td>2.630306</td>\n",
       "      <td>13</td>\n",
       "      <td>0.078607</td>\n",
       "      <td>0.016092</td>\n",
       "      <td>0.538462</td>\n",
       "      <td>0.204714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1154057</th>\n",
       "      <td>france</td>\n",
       "      <td>bourbons</td>\n",
       "      <td>5</td>\n",
       "      <td>0.030233</td>\n",
       "      <td>435</td>\n",
       "      <td>2.630306</td>\n",
       "      <td>10</td>\n",
       "      <td>0.060467</td>\n",
       "      <td>0.011494</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.190092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1154535</th>\n",
       "      <td>france</td>\n",
       "      <td>embassy</td>\n",
       "      <td>4</td>\n",
       "      <td>0.024187</td>\n",
       "      <td>435</td>\n",
       "      <td>2.630306</td>\n",
       "      <td>10</td>\n",
       "      <td>0.060467</td>\n",
       "      <td>0.009195</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.152074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1155310</th>\n",
       "      <td>france</td>\n",
       "      <td>montmorency</td>\n",
       "      <td>3</td>\n",
       "      <td>0.018140</td>\n",
       "      <td>435</td>\n",
       "      <td>2.630306</td>\n",
       "      <td>8</td>\n",
       "      <td>0.048373</td>\n",
       "      <td>0.006897</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.142569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1154056</th>\n",
       "      <td>france</td>\n",
       "      <td>bourbon</td>\n",
       "      <td>6</td>\n",
       "      <td>0.036280</td>\n",
       "      <td>435</td>\n",
       "      <td>2.630306</td>\n",
       "      <td>16</td>\n",
       "      <td>0.096747</td>\n",
       "      <td>0.013793</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.142569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1154486</th>\n",
       "      <td>france</td>\n",
       "      <td>dubois</td>\n",
       "      <td>4</td>\n",
       "      <td>0.024187</td>\n",
       "      <td>435</td>\n",
       "      <td>2.630306</td>\n",
       "      <td>11</td>\n",
       "      <td>0.066513</td>\n",
       "      <td>0.009195</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>0.138249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1155704</th>\n",
       "      <td>france</td>\n",
       "      <td>regency</td>\n",
       "      <td>5</td>\n",
       "      <td>0.030233</td>\n",
       "      <td>435</td>\n",
       "      <td>2.630306</td>\n",
       "      <td>15</td>\n",
       "      <td>0.090700</td>\n",
       "      <td>0.011494</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.126728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1154350</th>\n",
       "      <td>france</td>\n",
       "      <td>de valois</td>\n",
       "      <td>4</td>\n",
       "      <td>0.024187</td>\n",
       "      <td>435</td>\n",
       "      <td>2.630306</td>\n",
       "      <td>12</td>\n",
       "      <td>0.072560</td>\n",
       "      <td>0.009195</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.126728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1154718</th>\n",
       "      <td>france</td>\n",
       "      <td>fontainebleau</td>\n",
       "      <td>4</td>\n",
       "      <td>0.024187</td>\n",
       "      <td>435</td>\n",
       "      <td>2.630306</td>\n",
       "      <td>12</td>\n",
       "      <td>0.072560</td>\n",
       "      <td>0.009195</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.126728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1154814</th>\n",
       "      <td>france</td>\n",
       "      <td>germain</td>\n",
       "      <td>4</td>\n",
       "      <td>0.024187</td>\n",
       "      <td>435</td>\n",
       "      <td>2.630306</td>\n",
       "      <td>13</td>\n",
       "      <td>0.078607</td>\n",
       "      <td>0.009195</td>\n",
       "      <td>0.307692</td>\n",
       "      <td>0.116980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1154308</th>\n",
       "      <td>france</td>\n",
       "      <td>credited</td>\n",
       "      <td>4</td>\n",
       "      <td>0.024187</td>\n",
       "      <td>435</td>\n",
       "      <td>2.630306</td>\n",
       "      <td>13</td>\n",
       "      <td>0.078607</td>\n",
       "      <td>0.009195</td>\n",
       "      <td>0.307692</td>\n",
       "      <td>0.116980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1155304</th>\n",
       "      <td>france</td>\n",
       "      <td>monarchy</td>\n",
       "      <td>4</td>\n",
       "      <td>0.024187</td>\n",
       "      <td>435</td>\n",
       "      <td>2.630306</td>\n",
       "      <td>13</td>\n",
       "      <td>0.078607</td>\n",
       "      <td>0.009195</td>\n",
       "      <td>0.307692</td>\n",
       "      <td>0.116980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1155828</th>\n",
       "      <td>france</td>\n",
       "      <td>sanctity</td>\n",
       "      <td>4</td>\n",
       "      <td>0.024187</td>\n",
       "      <td>435</td>\n",
       "      <td>2.630306</td>\n",
       "      <td>13</td>\n",
       "      <td>0.078607</td>\n",
       "      <td>0.009195</td>\n",
       "      <td>0.307692</td>\n",
       "      <td>0.116980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1154161</th>\n",
       "      <td>france</td>\n",
       "      <td>chevalier de</td>\n",
       "      <td>3</td>\n",
       "      <td>0.018140</td>\n",
       "      <td>435</td>\n",
       "      <td>2.630306</td>\n",
       "      <td>10</td>\n",
       "      <td>0.060467</td>\n",
       "      <td>0.006897</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.114055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1155770</th>\n",
       "      <td>france</td>\n",
       "      <td>revolting</td>\n",
       "      <td>3</td>\n",
       "      <td>0.018140</td>\n",
       "      <td>435</td>\n",
       "      <td>2.630306</td>\n",
       "      <td>10</td>\n",
       "      <td>0.060467</td>\n",
       "      <td>0.006897</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.114055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1153963</th>\n",
       "      <td>france</td>\n",
       "      <td>averse</td>\n",
       "      <td>3</td>\n",
       "      <td>0.018140</td>\n",
       "      <td>435</td>\n",
       "      <td>2.630306</td>\n",
       "      <td>10</td>\n",
       "      <td>0.060467</td>\n",
       "      <td>0.006897</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.114055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1155011</th>\n",
       "      <td>france</td>\n",
       "      <td>infamy</td>\n",
       "      <td>3</td>\n",
       "      <td>0.018140</td>\n",
       "      <td>435</td>\n",
       "      <td>2.630306</td>\n",
       "      <td>10</td>\n",
       "      <td>0.060467</td>\n",
       "      <td>0.006897</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.114055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1154058</th>\n",
       "      <td>france</td>\n",
       "      <td>bourg</td>\n",
       "      <td>2</td>\n",
       "      <td>0.012093</td>\n",
       "      <td>435</td>\n",
       "      <td>2.630306</td>\n",
       "      <td>7</td>\n",
       "      <td>0.042327</td>\n",
       "      <td>0.004598</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.108624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1154798</th>\n",
       "      <td>france</td>\n",
       "      <td>gaston</td>\n",
       "      <td>2</td>\n",
       "      <td>0.012093</td>\n",
       "      <td>435</td>\n",
       "      <td>2.630306</td>\n",
       "      <td>7</td>\n",
       "      <td>0.042327</td>\n",
       "      <td>0.004598</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.108624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1155307</th>\n",
       "      <td>france</td>\n",
       "      <td>montague</td>\n",
       "      <td>2</td>\n",
       "      <td>0.012093</td>\n",
       "      <td>435</td>\n",
       "      <td>2.630306</td>\n",
       "      <td>7</td>\n",
       "      <td>0.042327</td>\n",
       "      <td>0.004598</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.108624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1155166</th>\n",
       "      <td>france</td>\n",
       "      <td>livres</td>\n",
       "      <td>5</td>\n",
       "      <td>0.030233</td>\n",
       "      <td>435</td>\n",
       "      <td>2.630306</td>\n",
       "      <td>18</td>\n",
       "      <td>0.108840</td>\n",
       "      <td>0.011494</td>\n",
       "      <td>0.277778</td>\n",
       "      <td>0.105607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1155923</th>\n",
       "      <td>france</td>\n",
       "      <td>si</td>\n",
       "      <td>3</td>\n",
       "      <td>0.018140</td>\n",
       "      <td>435</td>\n",
       "      <td>2.630306</td>\n",
       "      <td>11</td>\n",
       "      <td>0.066513</td>\n",
       "      <td>0.006897</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.103687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1155192</th>\n",
       "      <td>france</td>\n",
       "      <td>louis xvi</td>\n",
       "      <td>3</td>\n",
       "      <td>0.018140</td>\n",
       "      <td>435</td>\n",
       "      <td>2.630306</td>\n",
       "      <td>11</td>\n",
       "      <td>0.066513</td>\n",
       "      <td>0.006897</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.103687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1155209</th>\n",
       "      <td>france</td>\n",
       "      <td>luz</td>\n",
       "      <td>3</td>\n",
       "      <td>0.018140</td>\n",
       "      <td>435</td>\n",
       "      <td>2.630306</td>\n",
       "      <td>11</td>\n",
       "      <td>0.066513</td>\n",
       "      <td>0.006897</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.103687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1154211</th>\n",
       "      <td>france</td>\n",
       "      <td>comme</td>\n",
       "      <td>4</td>\n",
       "      <td>0.024187</td>\n",
       "      <td>435</td>\n",
       "      <td>2.630306</td>\n",
       "      <td>15</td>\n",
       "      <td>0.090700</td>\n",
       "      <td>0.009195</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.101382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1154491</th>\n",
       "      <td>france</td>\n",
       "      <td>dujarier</td>\n",
       "      <td>4</td>\n",
       "      <td>0.024187</td>\n",
       "      <td>435</td>\n",
       "      <td>2.630306</td>\n",
       "      <td>16</td>\n",
       "      <td>0.096747</td>\n",
       "      <td>0.009195</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.095046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1154425</th>\n",
       "      <td>france</td>\n",
       "      <td>diplomacy</td>\n",
       "      <td>4</td>\n",
       "      <td>0.024187</td>\n",
       "      <td>435</td>\n",
       "      <td>2.630306</td>\n",
       "      <td>16</td>\n",
       "      <td>0.096747</td>\n",
       "      <td>0.009195</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.095046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1155544</th>\n",
       "      <td>france</td>\n",
       "      <td>poignant</td>\n",
       "      <td>4</td>\n",
       "      <td>0.024187</td>\n",
       "      <td>435</td>\n",
       "      <td>2.630306</td>\n",
       "      <td>16</td>\n",
       "      <td>0.096747</td>\n",
       "      <td>0.009195</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.095046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1156322</th>\n",
       "      <td>france</td>\n",
       "      <td>we are</td>\n",
       "      <td>3</td>\n",
       "      <td>0.018140</td>\n",
       "      <td>435</td>\n",
       "      <td>2.630306</td>\n",
       "      <td>337</td>\n",
       "      <td>2.037731</td>\n",
       "      <td>0.006897</td>\n",
       "      <td>0.008902</td>\n",
       "      <td>0.003384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1154762</th>\n",
       "      <td>france</td>\n",
       "      <td>free</td>\n",
       "      <td>2</td>\n",
       "      <td>0.012093</td>\n",
       "      <td>435</td>\n",
       "      <td>2.630306</td>\n",
       "      <td>225</td>\n",
       "      <td>1.360503</td>\n",
       "      <td>0.004598</td>\n",
       "      <td>0.008889</td>\n",
       "      <td>0.003379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1154059</th>\n",
       "      <td>france</td>\n",
       "      <td>boy</td>\n",
       "      <td>4</td>\n",
       "      <td>0.024187</td>\n",
       "      <td>435</td>\n",
       "      <td>2.630306</td>\n",
       "      <td>457</td>\n",
       "      <td>2.763333</td>\n",
       "      <td>0.009195</td>\n",
       "      <td>0.008753</td>\n",
       "      <td>0.003328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1155309</th>\n",
       "      <td>france</td>\n",
       "      <td>month</td>\n",
       "      <td>3</td>\n",
       "      <td>0.018140</td>\n",
       "      <td>435</td>\n",
       "      <td>2.630306</td>\n",
       "      <td>347</td>\n",
       "      <td>2.098198</td>\n",
       "      <td>0.006897</td>\n",
       "      <td>0.008646</td>\n",
       "      <td>0.003287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1155200</th>\n",
       "      <td>france</td>\n",
       "      <td>loving</td>\n",
       "      <td>2</td>\n",
       "      <td>0.012093</td>\n",
       "      <td>435</td>\n",
       "      <td>2.630306</td>\n",
       "      <td>232</td>\n",
       "      <td>1.402830</td>\n",
       "      <td>0.004598</td>\n",
       "      <td>0.008621</td>\n",
       "      <td>0.003277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1153856</th>\n",
       "      <td>france</td>\n",
       "      <td>although</td>\n",
       "      <td>3</td>\n",
       "      <td>0.018140</td>\n",
       "      <td>435</td>\n",
       "      <td>2.630306</td>\n",
       "      <td>348</td>\n",
       "      <td>2.104245</td>\n",
       "      <td>0.006897</td>\n",
       "      <td>0.008621</td>\n",
       "      <td>0.003277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1153850</th>\n",
       "      <td>france</td>\n",
       "      <td>alone</td>\n",
       "      <td>4</td>\n",
       "      <td>0.024187</td>\n",
       "      <td>435</td>\n",
       "      <td>2.630306</td>\n",
       "      <td>465</td>\n",
       "      <td>2.811706</td>\n",
       "      <td>0.009195</td>\n",
       "      <td>0.008602</td>\n",
       "      <td>0.003270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1155906</th>\n",
       "      <td>france</td>\n",
       "      <td>shall</td>\n",
       "      <td>2</td>\n",
       "      <td>0.012093</td>\n",
       "      <td>435</td>\n",
       "      <td>2.630306</td>\n",
       "      <td>233</td>\n",
       "      <td>1.408877</td>\n",
       "      <td>0.004598</td>\n",
       "      <td>0.008584</td>\n",
       "      <td>0.003263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1155419</th>\n",
       "      <td>france</td>\n",
       "      <td>officer</td>\n",
       "      <td>2</td>\n",
       "      <td>0.012093</td>\n",
       "      <td>435</td>\n",
       "      <td>2.630306</td>\n",
       "      <td>235</td>\n",
       "      <td>1.420970</td>\n",
       "      <td>0.004598</td>\n",
       "      <td>0.008511</td>\n",
       "      <td>0.003236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1154822</th>\n",
       "      <td>france</td>\n",
       "      <td>giving</td>\n",
       "      <td>2</td>\n",
       "      <td>0.012093</td>\n",
       "      <td>435</td>\n",
       "      <td>2.630306</td>\n",
       "      <td>238</td>\n",
       "      <td>1.439110</td>\n",
       "      <td>0.004598</td>\n",
       "      <td>0.008403</td>\n",
       "      <td>0.003195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1155480</th>\n",
       "      <td>france</td>\n",
       "      <td>past</td>\n",
       "      <td>2</td>\n",
       "      <td>0.012093</td>\n",
       "      <td>435</td>\n",
       "      <td>2.630306</td>\n",
       "      <td>244</td>\n",
       "      <td>1.475390</td>\n",
       "      <td>0.004598</td>\n",
       "      <td>0.008197</td>\n",
       "      <td>0.003116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1154452</th>\n",
       "      <td>france</td>\n",
       "      <td>doctor</td>\n",
       "      <td>2</td>\n",
       "      <td>0.012093</td>\n",
       "      <td>435</td>\n",
       "      <td>2.630306</td>\n",
       "      <td>246</td>\n",
       "      <td>1.487483</td>\n",
       "      <td>0.004598</td>\n",
       "      <td>0.008130</td>\n",
       "      <td>0.003091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1154967</th>\n",
       "      <td>france</td>\n",
       "      <td>i do</td>\n",
       "      <td>2</td>\n",
       "      <td>0.012093</td>\n",
       "      <td>435</td>\n",
       "      <td>2.630306</td>\n",
       "      <td>247</td>\n",
       "      <td>1.493530</td>\n",
       "      <td>0.004598</td>\n",
       "      <td>0.008097</td>\n",
       "      <td>0.003078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1154497</th>\n",
       "      <td>france</td>\n",
       "      <td>duty</td>\n",
       "      <td>4</td>\n",
       "      <td>0.024187</td>\n",
       "      <td>435</td>\n",
       "      <td>2.630306</td>\n",
       "      <td>496</td>\n",
       "      <td>2.999153</td>\n",
       "      <td>0.009195</td>\n",
       "      <td>0.008065</td>\n",
       "      <td>0.003066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1154580</th>\n",
       "      <td>france</td>\n",
       "      <td>especially</td>\n",
       "      <td>2</td>\n",
       "      <td>0.012093</td>\n",
       "      <td>435</td>\n",
       "      <td>2.630306</td>\n",
       "      <td>249</td>\n",
       "      <td>1.505623</td>\n",
       "      <td>0.004598</td>\n",
       "      <td>0.008032</td>\n",
       "      <td>0.003054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1155644</th>\n",
       "      <td>france</td>\n",
       "      <td>purpose</td>\n",
       "      <td>2</td>\n",
       "      <td>0.012093</td>\n",
       "      <td>435</td>\n",
       "      <td>2.630306</td>\n",
       "      <td>249</td>\n",
       "      <td>1.505623</td>\n",
       "      <td>0.004598</td>\n",
       "      <td>0.008032</td>\n",
       "      <td>0.003054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1153801</th>\n",
       "      <td>france</td>\n",
       "      <td>act</td>\n",
       "      <td>2</td>\n",
       "      <td>0.012093</td>\n",
       "      <td>435</td>\n",
       "      <td>2.630306</td>\n",
       "      <td>259</td>\n",
       "      <td>1.566090</td>\n",
       "      <td>0.004598</td>\n",
       "      <td>0.007722</td>\n",
       "      <td>0.002936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1155676</th>\n",
       "      <td>france</td>\n",
       "      <td>reading</td>\n",
       "      <td>2</td>\n",
       "      <td>0.012093</td>\n",
       "      <td>435</td>\n",
       "      <td>2.630306</td>\n",
       "      <td>271</td>\n",
       "      <td>1.638650</td>\n",
       "      <td>0.004598</td>\n",
       "      <td>0.007380</td>\n",
       "      <td>0.002806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1155348</th>\n",
       "      <td>france</td>\n",
       "      <td>natural</td>\n",
       "      <td>2</td>\n",
       "      <td>0.012093</td>\n",
       "      <td>435</td>\n",
       "      <td>2.630306</td>\n",
       "      <td>279</td>\n",
       "      <td>1.687024</td>\n",
       "      <td>0.004598</td>\n",
       "      <td>0.007168</td>\n",
       "      <td>0.002725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1155467</th>\n",
       "      <td>france</td>\n",
       "      <td>parent</td>\n",
       "      <td>2</td>\n",
       "      <td>0.012093</td>\n",
       "      <td>435</td>\n",
       "      <td>2.630306</td>\n",
       "      <td>292</td>\n",
       "      <td>1.765631</td>\n",
       "      <td>0.004598</td>\n",
       "      <td>0.006849</td>\n",
       "      <td>0.002604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1156039</th>\n",
       "      <td>france</td>\n",
       "      <td>strength</td>\n",
       "      <td>2</td>\n",
       "      <td>0.012093</td>\n",
       "      <td>435</td>\n",
       "      <td>2.630306</td>\n",
       "      <td>295</td>\n",
       "      <td>1.783771</td>\n",
       "      <td>0.004598</td>\n",
       "      <td>0.006780</td>\n",
       "      <td>0.002578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1154144</th>\n",
       "      <td>france</td>\n",
       "      <td>change</td>\n",
       "      <td>2</td>\n",
       "      <td>0.012093</td>\n",
       "      <td>435</td>\n",
       "      <td>2.630306</td>\n",
       "      <td>297</td>\n",
       "      <td>1.795864</td>\n",
       "      <td>0.004598</td>\n",
       "      <td>0.006734</td>\n",
       "      <td>0.002560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1155613</th>\n",
       "      <td>france</td>\n",
       "      <td>probably</td>\n",
       "      <td>2</td>\n",
       "      <td>0.012093</td>\n",
       "      <td>435</td>\n",
       "      <td>2.630306</td>\n",
       "      <td>298</td>\n",
       "      <td>1.801911</td>\n",
       "      <td>0.004598</td>\n",
       "      <td>0.006711</td>\n",
       "      <td>0.002552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1156283</th>\n",
       "      <td>france</td>\n",
       "      <td>view</td>\n",
       "      <td>2</td>\n",
       "      <td>0.012093</td>\n",
       "      <td>435</td>\n",
       "      <td>2.630306</td>\n",
       "      <td>311</td>\n",
       "      <td>1.880518</td>\n",
       "      <td>0.004598</td>\n",
       "      <td>0.006431</td>\n",
       "      <td>0.002445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1155261</th>\n",
       "      <td>france</td>\n",
       "      <td>meeting</td>\n",
       "      <td>3</td>\n",
       "      <td>0.018140</td>\n",
       "      <td>435</td>\n",
       "      <td>2.630306</td>\n",
       "      <td>477</td>\n",
       "      <td>2.884267</td>\n",
       "      <td>0.006897</td>\n",
       "      <td>0.006289</td>\n",
       "      <td>0.002391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1155291</th>\n",
       "      <td>france</td>\n",
       "      <td>miss</td>\n",
       "      <td>3</td>\n",
       "      <td>0.018140</td>\n",
       "      <td>435</td>\n",
       "      <td>2.630306</td>\n",
       "      <td>504</td>\n",
       "      <td>3.047527</td>\n",
       "      <td>0.006897</td>\n",
       "      <td>0.005952</td>\n",
       "      <td>0.002263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1154519</th>\n",
       "      <td>france</td>\n",
       "      <td>effort</td>\n",
       "      <td>2</td>\n",
       "      <td>0.012093</td>\n",
       "      <td>435</td>\n",
       "      <td>2.630306</td>\n",
       "      <td>356</td>\n",
       "      <td>2.152618</td>\n",
       "      <td>0.004598</td>\n",
       "      <td>0.005618</td>\n",
       "      <td>0.002136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1155163</th>\n",
       "      <td>france</td>\n",
       "      <td>live</td>\n",
       "      <td>2</td>\n",
       "      <td>0.012093</td>\n",
       "      <td>435</td>\n",
       "      <td>2.630306</td>\n",
       "      <td>363</td>\n",
       "      <td>2.194945</td>\n",
       "      <td>0.004598</td>\n",
       "      <td>0.005510</td>\n",
       "      <td>0.002095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1155252</th>\n",
       "      <td>france</td>\n",
       "      <td>may be</td>\n",
       "      <td>2</td>\n",
       "      <td>0.012093</td>\n",
       "      <td>435</td>\n",
       "      <td>2.630306</td>\n",
       "      <td>372</td>\n",
       "      <td>2.249365</td>\n",
       "      <td>0.004598</td>\n",
       "      <td>0.005376</td>\n",
       "      <td>0.002044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1155329</th>\n",
       "      <td>france</td>\n",
       "      <td>mr.</td>\n",
       "      <td>2</td>\n",
       "      <td>0.012093</td>\n",
       "      <td>435</td>\n",
       "      <td>2.630306</td>\n",
       "      <td>643</td>\n",
       "      <td>3.888015</td>\n",
       "      <td>0.004598</td>\n",
       "      <td>0.003110</td>\n",
       "      <td>0.001183</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2684 rows  11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         item_A         item_B  freqAB  supportAB  freqA  supportA  freqB  \\\n",
       "1155232  france  maria-theresa       6   0.036280    435  2.630306     10   \n",
       "1154266  france          conti       6   0.036280    435  2.630306     10   \n",
       "1154237  france         cond       7   0.042327    435  2.630306     13   \n",
       "1154057  france       bourbons       5   0.030233    435  2.630306     10   \n",
       "1154535  france        embassy       4   0.024187    435  2.630306     10   \n",
       "1155310  france    montmorency       3   0.018140    435  2.630306      8   \n",
       "1154056  france        bourbon       6   0.036280    435  2.630306     16   \n",
       "1154486  france         dubois       4   0.024187    435  2.630306     11   \n",
       "1155704  france        regency       5   0.030233    435  2.630306     15   \n",
       "1154350  france      de valois       4   0.024187    435  2.630306     12   \n",
       "1154718  france  fontainebleau       4   0.024187    435  2.630306     12   \n",
       "1154814  france        germain       4   0.024187    435  2.630306     13   \n",
       "1154308  france       credited       4   0.024187    435  2.630306     13   \n",
       "1155304  france       monarchy       4   0.024187    435  2.630306     13   \n",
       "1155828  france       sanctity       4   0.024187    435  2.630306     13   \n",
       "1154161  france   chevalier de       3   0.018140    435  2.630306     10   \n",
       "1155770  france      revolting       3   0.018140    435  2.630306     10   \n",
       "1153963  france         averse       3   0.018140    435  2.630306     10   \n",
       "1155011  france         infamy       3   0.018140    435  2.630306     10   \n",
       "1154058  france          bourg       2   0.012093    435  2.630306      7   \n",
       "1154798  france         gaston       2   0.012093    435  2.630306      7   \n",
       "1155307  france       montague       2   0.012093    435  2.630306      7   \n",
       "1155166  france         livres       5   0.030233    435  2.630306     18   \n",
       "1155923  france             si       3   0.018140    435  2.630306     11   \n",
       "1155192  france      louis xvi       3   0.018140    435  2.630306     11   \n",
       "1155209  france            luz       3   0.018140    435  2.630306     11   \n",
       "1154211  france          comme       4   0.024187    435  2.630306     15   \n",
       "1154491  france       dujarier       4   0.024187    435  2.630306     16   \n",
       "1154425  france      diplomacy       4   0.024187    435  2.630306     16   \n",
       "1155544  france       poignant       4   0.024187    435  2.630306     16   \n",
       "...         ...            ...     ...        ...    ...       ...    ...   \n",
       "1156322  france         we are       3   0.018140    435  2.630306    337   \n",
       "1154762  france           free       2   0.012093    435  2.630306    225   \n",
       "1154059  france            boy       4   0.024187    435  2.630306    457   \n",
       "1155309  france          month       3   0.018140    435  2.630306    347   \n",
       "1155200  france         loving       2   0.012093    435  2.630306    232   \n",
       "1153856  france       although       3   0.018140    435  2.630306    348   \n",
       "1153850  france          alone       4   0.024187    435  2.630306    465   \n",
       "1155906  france          shall       2   0.012093    435  2.630306    233   \n",
       "1155419  france        officer       2   0.012093    435  2.630306    235   \n",
       "1154822  france         giving       2   0.012093    435  2.630306    238   \n",
       "1155480  france           past       2   0.012093    435  2.630306    244   \n",
       "1154452  france         doctor       2   0.012093    435  2.630306    246   \n",
       "1154967  france           i do       2   0.012093    435  2.630306    247   \n",
       "1154497  france           duty       4   0.024187    435  2.630306    496   \n",
       "1154580  france     especially       2   0.012093    435  2.630306    249   \n",
       "1155644  france        purpose       2   0.012093    435  2.630306    249   \n",
       "1153801  france            act       2   0.012093    435  2.630306    259   \n",
       "1155676  france        reading       2   0.012093    435  2.630306    271   \n",
       "1155348  france        natural       2   0.012093    435  2.630306    279   \n",
       "1155467  france         parent       2   0.012093    435  2.630306    292   \n",
       "1156039  france       strength       2   0.012093    435  2.630306    295   \n",
       "1154144  france         change       2   0.012093    435  2.630306    297   \n",
       "1155613  france       probably       2   0.012093    435  2.630306    298   \n",
       "1156283  france           view       2   0.012093    435  2.630306    311   \n",
       "1155261  france        meeting       3   0.018140    435  2.630306    477   \n",
       "1155291  france           miss       3   0.018140    435  2.630306    504   \n",
       "1154519  france         effort       2   0.012093    435  2.630306    356   \n",
       "1155163  france           live       2   0.012093    435  2.630306    363   \n",
       "1155252  france         may be       2   0.012093    435  2.630306    372   \n",
       "1155329  france            mr.       2   0.012093    435  2.630306    643   \n",
       "\n",
       "         supportB  confidenceAtoB  confidenceBtoA      lift  \n",
       "1155232  0.060467        0.013793        0.600000  0.228110  \n",
       "1154266  0.060467        0.013793        0.600000  0.228110  \n",
       "1154237  0.078607        0.016092        0.538462  0.204714  \n",
       "1154057  0.060467        0.011494        0.500000  0.190092  \n",
       "1154535  0.060467        0.009195        0.400000  0.152074  \n",
       "1155310  0.048373        0.006897        0.375000  0.142569  \n",
       "1154056  0.096747        0.013793        0.375000  0.142569  \n",
       "1154486  0.066513        0.009195        0.363636  0.138249  \n",
       "1155704  0.090700        0.011494        0.333333  0.126728  \n",
       "1154350  0.072560        0.009195        0.333333  0.126728  \n",
       "1154718  0.072560        0.009195        0.333333  0.126728  \n",
       "1154814  0.078607        0.009195        0.307692  0.116980  \n",
       "1154308  0.078607        0.009195        0.307692  0.116980  \n",
       "1155304  0.078607        0.009195        0.307692  0.116980  \n",
       "1155828  0.078607        0.009195        0.307692  0.116980  \n",
       "1154161  0.060467        0.006897        0.300000  0.114055  \n",
       "1155770  0.060467        0.006897        0.300000  0.114055  \n",
       "1153963  0.060467        0.006897        0.300000  0.114055  \n",
       "1155011  0.060467        0.006897        0.300000  0.114055  \n",
       "1154058  0.042327        0.004598        0.285714  0.108624  \n",
       "1154798  0.042327        0.004598        0.285714  0.108624  \n",
       "1155307  0.042327        0.004598        0.285714  0.108624  \n",
       "1155166  0.108840        0.011494        0.277778  0.105607  \n",
       "1155923  0.066513        0.006897        0.272727  0.103687  \n",
       "1155192  0.066513        0.006897        0.272727  0.103687  \n",
       "1155209  0.066513        0.006897        0.272727  0.103687  \n",
       "1154211  0.090700        0.009195        0.266667  0.101382  \n",
       "1154491  0.096747        0.009195        0.250000  0.095046  \n",
       "1154425  0.096747        0.009195        0.250000  0.095046  \n",
       "1155544  0.096747        0.009195        0.250000  0.095046  \n",
       "...           ...             ...             ...       ...  \n",
       "1156322  2.037731        0.006897        0.008902  0.003384  \n",
       "1154762  1.360503        0.004598        0.008889  0.003379  \n",
       "1154059  2.763333        0.009195        0.008753  0.003328  \n",
       "1155309  2.098198        0.006897        0.008646  0.003287  \n",
       "1155200  1.402830        0.004598        0.008621  0.003277  \n",
       "1153856  2.104245        0.006897        0.008621  0.003277  \n",
       "1153850  2.811706        0.009195        0.008602  0.003270  \n",
       "1155906  1.408877        0.004598        0.008584  0.003263  \n",
       "1155419  1.420970        0.004598        0.008511  0.003236  \n",
       "1154822  1.439110        0.004598        0.008403  0.003195  \n",
       "1155480  1.475390        0.004598        0.008197  0.003116  \n",
       "1154452  1.487483        0.004598        0.008130  0.003091  \n",
       "1154967  1.493530        0.004598        0.008097  0.003078  \n",
       "1154497  2.999153        0.009195        0.008065  0.003066  \n",
       "1154580  1.505623        0.004598        0.008032  0.003054  \n",
       "1155644  1.505623        0.004598        0.008032  0.003054  \n",
       "1153801  1.566090        0.004598        0.007722  0.002936  \n",
       "1155676  1.638650        0.004598        0.007380  0.002806  \n",
       "1155348  1.687024        0.004598        0.007168  0.002725  \n",
       "1155467  1.765631        0.004598        0.006849  0.002604  \n",
       "1156039  1.783771        0.004598        0.006780  0.002578  \n",
       "1154144  1.795864        0.004598        0.006734  0.002560  \n",
       "1155613  1.801911        0.004598        0.006711  0.002552  \n",
       "1156283  1.880518        0.004598        0.006431  0.002445  \n",
       "1155261  2.884267        0.006897        0.006289  0.002391  \n",
       "1155291  3.047527        0.006897        0.005952  0.002263  \n",
       "1154519  2.152618        0.004598        0.005618  0.002136  \n",
       "1155163  2.194945        0.004598        0.005510  0.002095  \n",
       "1155252  2.249365        0.004598        0.005376  0.002044  \n",
       "1155329  3.888015        0.004598        0.003110  0.001183  \n",
       "\n",
       "[2684 rows x 11 columns]"
      ]
     },
     "execution_count": 328,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rules[rules.item_A == 'france']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'value' : [1,2,None,5,None,5,6,None,None,10,1]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [],
   "source": [
    "reverse_fill = df.iloc[::-1].value.fillna(method = 'ffill')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [],
   "source": [
    "fill = df.value.fillna(method = 'ffill')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      1.0\n",
       "1      2.0\n",
       "2      3.5\n",
       "3      5.0\n",
       "4      5.0\n",
       "5      5.0\n",
       "6      6.0\n",
       "7      8.0\n",
       "8      8.0\n",
       "9     10.0\n",
       "10     1.0\n",
       "dtype: float64"
      ]
     },
     "execution_count": 375,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.concat([fill,reverse_fill],axis = 1).apply(lambda x: np.mean(x), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
